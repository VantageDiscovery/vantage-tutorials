{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cebeda1-8b38-4646-b210-2f9098160814",
   "metadata": {},
   "source": [
    "# BlogArticles\n",
    "\n",
    "Download the following set of articles from here:\n",
    "(HuggingFace  Articles)[https://huggingface.co/datasets/fabiochiu/medium-articles/blob/main/medium_articles.csv]\n",
    "\n",
    "Unzip urls_preview_images_json_files.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687601e-e76e-461f-8624-6d87c16b561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Pattern to match your JSON files\n",
    "file_pattern = 'urls_preview_images*.json'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# List to hold each DataFrame\n",
    "dfs = []\n",
    "   \n",
    "for file in files:\n",
    "    json_data = read_json_file(file)\n",
    "    df = pd.DataFrame(json_data)\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# You now have a single DataFrame `combined_df` containing all the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637e579-d564-426f-bda6-3a27836ef3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = 'medium_articles.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to convert string representation of list to an actual list\n",
    "def convert_string_to_list(string):\n",
    "    return eval(string)\n",
    "\n",
    "# Apply the function to the 'authors' and 'tags' columns\n",
    "df['authors'] = df['authors'].apply(convert_string_to_list)\n",
    "df['tags'] = df['tags'].apply(convert_string_to_list)\n",
    "\n",
    "# Show the DataFrame to verify the changes\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19882e79-c318-4419-96c7-7780f0faf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Function to create the MD5 hash\n",
    "def create_md5_hash(url):\n",
    "    return hashlib.md5(url.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def create_id(url, title, authors):\n",
    "    \n",
    "    id_str = f\"{create_md5_hash(url)}\"\n",
    "\n",
    "    # Remove newlines and other control characters\n",
    "    id_str = re.sub(r'[\\r\\n\\t ]+', '+', id_str)\n",
    "\n",
    "    # Ensure the id is not longer than 255 characters and convert to Unicode\n",
    "    return id_str[:255]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Update the 'id' and 'text' columns\n",
    "df['id'] = df.apply(lambda row: create_id(row['url'], row['title'], row['authors']), axis=1)\n",
    "df['text'] = df['text'].apply(lambda x: x[:20000].replace('endoftext', 'REDACTED')) ## bug\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create 'noop_title' column\n",
    "#df['noop_title'] = df['title'].apply(lambda x: x[:64])\n",
    "#df['noop_description'] = df['text'].apply(lambda x: x[:128])\n",
    "\n",
    "df['noop_title'] = df['title'].apply(lambda x: str(x)[:64] if pd.notnull(x) else x)\n",
    "df['noop_description'] = df['text'].apply(lambda x: str(x)[:128] if pd.notnull(x) else x)\n",
    "df['noop_url'] = df['url']\n",
    "#df['noop_image_url'] = 'https://miro.medium.com/v2/resize:fit:1200/0*mFed_WBqkegFJXgx'\n",
    "#df['noop_publishing_authors'] = df['authors'].apply(lambda x: str(x)[:65] if pd.notnull(x) else x)\n",
    "df['noop_publishing_authors'] = df['authors'].apply(lambda x: ', '.join(x) if isinstance(x, list) and len(x) > 0 else '')\n",
    "df['noop_timestamp'] = df['timestamp']\n",
    "\n",
    "# Merge the main df with the combined_df\n",
    "merged_df = pd.merge(df, combined_df[['url', 'preview_image']], on='url', how='left')\n",
    "\n",
    "# Default image URL\n",
    "default_image_url = 'https://miro.medium.com/v2/resize:fit:1200/0*mFed_WBqkegFJXgx'\n",
    "\n",
    "# Update noop_image_url with preview_image, use default where preview_image is missing\n",
    "merged_df['noop_image_url'] = merged_df['preview_image'].fillna(default_image_url)\n",
    "\n",
    "# Drop the extra preview_image column after updating\n",
    "merged_df.drop('preview_image', axis=1, inplace=True)\n",
    "\n",
    "# Rename and select columns\n",
    "merged_df.rename(columns={'tags': 'meta_tag', 'authors': 'meta_authors'}, inplace=True)\n",
    "output_df = merged_df[['id', 'text', 'meta_tag', 'meta_authors', 'noop_title', 'noop_description', 'noop_url', 'noop_image_url', 'noop_publishing_authors', 'noop_timestamp']]\n",
    "\n",
    "# Output the DataFrame in chunks of 10000 rows\n",
    "chunk_size = 10000\n",
    "for i in range(0, len(output_df), chunk_size):\n",
    "    chunk_df = output_df.iloc[i:i+chunk_size]\n",
    "    chunk_file_path = f'vantage_blog_chunk_{i//chunk_size}.parquet'\n",
    "    chunk_df.to_parquet(chunk_file_path, index=False)\n",
    "    print(f\"Chunk {i//chunk_size} written to {chunk_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
